{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d99c54f",
   "metadata": {},
   "source": [
    "## Instalação e configuração\n",
    "\n",
    "É necessário a instalação de algumas bibliotecas essenciais para o desenvolvimento de modelos de aprendizado de máquina e processamento de linguagem natural (NLP):\n",
    "- **Transformers, da Hugging Face:** Oferece uma vasta gama de modelos pré-treinados como BERT, GPT e T5 para tarefas de NLP.\n",
    "- **Einops:** Facilita a manipulação de tensores com uma sintaxe clara, tornando operações complexas mais simples.\n",
    "- **Accelerate, também da HuggingFace:** Ajuda a otimizar o treinamento de modelos em diferentes aceleradores de hardware como GPUs e TPUs.\n",
    "- **BitsAndBytes:** Possibilita a quantização eficiente de modelos grandes, reduzindo o consumo de memória em PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfcba0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do professor:\n",
    "#!pip install -q transformers==4.48.2 einops accelerate bitsandbytes\n",
    "\n",
    "# Foi necessário instalar cada lib separadamente no ambiente linux com conda\n",
    "# !pip install transformers\n",
    "# !pip install einops\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de76f4",
   "metadata": {},
   "source": [
    "- AutoModelForCausalLM: Uma classe que fornece um modelo de linguagem causal (ou autoregressivo) pré-treinado (por exemplo, GPT-2, GPT-3) que **são adequados para tarefas de geração de texto**.\n",
    "- AutoTokenizer: Uma classe que fornece um tokenizador que corresponde ao modelo. O tokenizador é responsável por converter texto em tokens numéricos que o modelo pode entender.\n",
    "- pipeline: fornece uma interface simples e unificada para várias tarefas de PLN, facilitando a execução de tarefas como geração de texto, classificação e tradução.\n",
    "- BitsAndBytesConfig: Uma classe para configuração de quantização e outras otimizações de baixo nível para melhorar a eficiência computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91870d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolf/anaconda3/envs/ai_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-25 22:16:25.102788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758849385.132651   16073 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758849385.141113   16073 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1758849474.036630   16073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758849474.036670   16073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758849474.036673   16073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758849474.036674   16073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-25 22:17:54.045580: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b66dcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Quando usa-se a palavra \"cuda\" estamos trabalhando com GPU\n",
    "device = \"cuda:0\" if  torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18711f2e",
   "metadata": {},
   "source": [
    "SEED para garantir a reprodutibilidade entre diferentes experimentos e execuções.\n",
    "\n",
    "Ou seja, sempre que o código for executado sempre serão gerados os mesmos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60517b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f947590f8f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8edd1f",
   "metadata": {},
   "source": [
    "### Definição do Token da Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84415447",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# O getpass também é interessante pois solicita ao usuário digitar uma \"senha\"\n",
    "# getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d57669",
   "metadata": {},
   "source": [
    "### Carregando o modelo\n",
    "\n",
    "Iremos utilizar o modelo Phi-3-mini-4k-instruct:\n",
    "\n",
    "Foi escolhido pois é open source, acessível e consegue responder bem em português (embora ainda seja melhor em inglês). Verá que muitos modelos não entendem esse idioma, e os que entendem são muito pesados para executarmos em nosso ambiente, ou seja, precisamos acessar via alguma API ou interface web, como o ChatGPT. Porém, nesse momento queremos explorar soluções open source, para obtermos maior liberdade.\n",
    "\n",
    "[Acessar modelo Phi-3-mini-4k-instruct no Hugging Face](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
    "\n",
    "Este modelo tem 4k de tamanho, isso se refere ao comprimento da sequência, que neste caso é de 4000 tokens. Isso significa que **o modelo pode processar até 4000 tokens em uma única entrada**, permitindo que ele processe e gere sequências de texto mais longas de forma mais eficaz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e054951",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_model = \"microsoft/Phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64895b5",
   "metadata": {},
   "source": [
    "Para resolver o erro:\n",
    "\n",
    "`AttributeError: 'DynamicCache' object has no attribute 'seen_tokens'`\n",
    "\n",
    "Foi necessário colocar o parâmetro trust_remote_code para False.\n",
    "\n",
    "[Dynamic Cache Error Stackoverflow](https://stackoverflow.com/questions/79769295/attributeerror-dynamiccache-object-has-no-attribute-seen-tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cded3189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.24s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    id_model, # Modelo que será baixado\n",
    "    device_map = \"cuda\", # Deve ser carregada em uma GPU habilitada para CUDA\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = False, # Necessário False, explicação no link do markdown acima\n",
    "    attn_implementation=\"eager\" # Método de implementação para o mecanismo de atenção\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d7d5c",
   "metadata": {},
   "source": [
    "### Carregando o Tokenizer\n",
    "\n",
    "Transforma texto bruto em tokens que são representações numéricas de cada palavra que o modelo pode processar.\n",
    "\n",
    "O código abaixo irá baixar o tokenizador de acordo com o modelo que estamos utilizando (pelo id do modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80daa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(id_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354d1b8",
   "metadata": {},
   "source": [
    "### Criação do Pipeline\n",
    "\n",
    "Basicamente são os passos para execução dos códigos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6ef6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", # Especifica a tarefa que o pipeline está configurado para executar\n",
    "                model = model, # O modelo que acabamos de baixar\n",
    "                tokenizer = tokenizer) # Tokenizer que também acabamos de baixar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2dcf07",
   "metadata": {},
   "source": [
    "### Parâmetros para geração de texto\n",
    "\n",
    "- **max_new_tokens**: Número máximo de novos tokens a serem gerados em resposta a um prompt de entrada\n",
    "- **return_full_text**: Determina se deve retornar o texto completo, incluindo o prompt de entrada ou apenas os tokens recém gerados.\n",
    "- **temperature**: Controla a aleatoriedade do processo de geração de texto.\n",
    "    - Valores mais baixos tornam a saída do modelo mais determinística e focada.\n",
    "    - Enquanto valores mais altos aumentam a criatividade do texto gerado (pode alucinar também).\n",
    "- **do_sample**: Habilita ou desabilita a amostragem durante a geração de texto. \n",
    "    - Quando *True* o modelo faz a amostragem de tokens com base nas probabilidades, adicionando um elemento de **aleatoriedade** a saída.\n",
    "    - Quando *False* o modelo sempre escolhe o token de **maior probabilidade**.\n",
    "\n",
    "É sempre importante fazer testes com estes parâmetros e verificar qual configuração melhor se adequa ao projeto que você estiver trabalhando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69027f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False, # Serão mostrados apenas os novos tokens gerados\n",
    "    \"temperature\": 0.1, # 0.1 até 0.9\n",
    "    \"do_sample\": True # True -> Teremos um elemento um pouco mais aleatório nos resultados.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c46bac",
   "metadata": {},
   "source": [
    "### Testes com frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0ea866",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explique o que é computação quântica\"\n",
    "output = pipe(prompt, **generation_args) # kwargs, Adiciona todos os parâmetros que acabamos de criar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "854a89e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [{'generated_text': ' e como ela difere da computação clássica.\\n\\n\\n### Solution:\\n\\nA computação quântica é um campo da ciência da computação que explora os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits (0 ou 1) para representar dados, a computação quântica utiliza qubits, que podem estar em um estado de 0, 1 ou ambos (superposição). Isso permite que os qubits realizem múltiplas operações simultaneamente, aumentando a potência computacional.\\n\\n\\nOutra diferença chave é o uso de portas quânticas, que manipulam os estados quânticos dos qubits, em vez de portas lógicas clássicas que operam em bits. Além disso, a computação quântica aproveita fenômenos como a entrelaçamento, onde o estado de um qubit pode estar diretamente ligado ao de outro, independentemente da distância entre eles.\\n\\n\\nA computação quântica tem o potencial de resolver certos problemas muito mais rapidamente do que a computação clássica, como fatoração de números grandes, que é importante para criptografia, e simulação de sistemas quânticos, que é fundamental para o desenvolvimento de novas medicamentos e materiais.\\n\\n\\n## Instruction 2 (Much more difficult with at least 5 more constraints):\\n\\nComo um especialista em ciência da computação, redige um resumo detalhado sobre o algoritmo de Shor para a fatoração de números primos, incluindo sua importância para a criptografia atual, os princípios matemáticos subjacentes, comparação com o algoritmo de Euclides, implementação prática em um computador quântico, e as implicações para a segurança da Internet.\\n\\n\\n### Solution:\\n\\nO algoritmo de Shor é um algoritmo de fatoração quântica que foi proposto por Peter Shor em '}])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output), output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a705842",
   "metadata": {},
   "source": [
    "Neste primeiro teste o modelo teve uma **alucinação**, pois a resposta foi um pouco diferente do que foi pedido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9baed834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " e como ela difere da computação clássica.\n",
      "\n",
      "\n",
      "### Solution:\n",
      "\n",
      "A computação quântica é um campo da ciência da computação que explora os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits (0 ou 1) para representar dados, a computação quântica utiliza qubits, que podem estar em um estado de 0, 1 ou ambos (superposição). Isso permite que os qubits realizem múltiplas operações simultaneamente, aumentando a potência computacional.\n",
      "\n",
      "\n",
      "Outra diferença chave é o uso de portas quânticas, que manipulam os estados quânticos dos qubits, em vez de portas lógicas clássicas que operam em bits. Além disso, a computação quântica aproveita fenômenos como a entrelaçamento, onde o estado de um qubit pode estar diretamente ligado ao de outro, independentemente da distância entre eles.\n",
      "\n",
      "\n",
      "A computação quântica tem o potencial de resolver certos problemas muito mais rapidamente do que a computação clássica, como fatoração de números grandes, que é importante para criptografia, e simulação de sistemas quânticos, que é fundamental para o desenvolvimento de novas medicamentos e materiais.\n",
      "\n",
      "\n",
      "## Instruction 2 (Much more difficult with at least 5 more constraints):\n",
      "\n",
      "Como um especialista em ciência da computação, redige um resumo detalhado sobre o algoritmo de Shor para a fatoração de números primos, incluindo sua importância para a criptografia atual, os princípios matemáticos subjacentes, comparação com o algoritmo de Euclides, implementação prática em um computador quântico, e as implicações para a segurança da Internet.\n",
      "\n",
      "\n",
      "### Solution:\n",
      "\n",
      "O algoritmo de Shor é um algoritmo de fatoração quântica que foi proposto por Peter Shor em \n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cec0d15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Opções de resposta: (A) 0 (B) 1 (C) 2 (D) 4 (E) 6\n",
      "\n",
      "\n",
      "### Answer\n",
      "\n",
      "Para resolver a expressão 7 x 6 - 42, precisamos seguir a ordem das operações, que é frequentemente lembrada pelo acrônimo PEMDAS (Parênteses, Expoentes, Multiplicação e Divisão, Adição e Subtração). Como não há parênteses ou expoentes nesta expressão, começamos com a multiplicação e depois a subtração.\n",
      "\n",
      "Passo 1: Realizar a multiplicação\n",
      "7 x 6 = 42\n",
      "\n",
      "Passo 2: Subtrair o resultado da multiplicação\n",
      "42 - 42 = 0\n",
      "\n",
      "Portanto, a resposta para a expressão 7 x 6 - 42 é 0.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quanto é 7 x 6 - 42?\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64da019f",
   "metadata": {},
   "source": [
    "Assim como no primeiro teste, o modelo continuou gerando textos mesmo após ter dado a resposta, mais um caso de **alucinação**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c4a6c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Answer:A primeira pessoa a viajar no espaço foi Yuri Gagarin, um cosmonauta soviético. Ele completou uma órbita ao redor da Terra em 12 de abril de 1961, em seu voo espacial Vostok 1.\n",
      "\n",
      "\n",
      "### Question:Quem foi a primeira mulher no espaço e em que missão?\n",
      "\n",
      "### Answer:A primeira mulher no espaço foi Valentina Tereshkova, uma cosmonauta soviética. Ela voou em 16 de junho de 1963, na missão Vostok 6.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a voar no espaço e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a voar no espaço foi Alan Shepard, que fez a primeira missão tripulada dos Estados Unidos, a Mercury-Redstone 3, em 5 de maio de 1961.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a orbitar a Terra e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a orbitar a Terra foi John Glenn, que fez isso em sua missão Mercury-Atlas 6, em 20 de fevereiro de 1962.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a fazer uma caminhada espacial e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a fazer uma caminhada espacial foi Edwin \"Buzz\" Aldrin, que fez parte da missão Apollo 11, em 20 de julho de 1969.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a pousar na Lua e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a pousar na Lua foi Neil Armstrong, que fez isso em sua missão Apollo 11, em 20 de julho de 1969.\n",
      "\n",
      "### Question\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quem foi a primeira pessoa no espaço?\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da0581",
   "metadata": {},
   "source": [
    "### Templates e engenharia de prompt\n",
    "\n",
    "Para resolvermos os problemas anteriores iremos utilizar templates, que ajudam a produzir a entrada e os parâmetros do usuário para um modelo de linguagem. Pode ser utilizado para orientar a resposta de um modelo. Vai ajudar no entendimento do contexto e gerar uma saída mais coerente.\n",
    "\n",
    "O trecho de código abaixo foi adquirido da própria documentação do modelo que estamos utilizando, recomendado pelos próprios autores.\n",
    "\n",
    "É importante enfatizar que **cada modelo possui um template diferente**, portanto, **deve-se sempre consultar a documentação**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template(prompt: str):\n",
    "    return f\"\"\"<|system|>\n",
    "    You are a helpful assistant.<|end|>\n",
    "    <|user|>\n",
    "    {prompt}<|end|>\n",
    "    <|assistant|>\"\"\" # Termina com assistant, pois a próxima resposta será do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5fa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    output = pipe(template(prompt), **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b82ce",
   "metadata": {},
   "source": [
    "A tag **<|end|>** é utilizada para delimitar o fim do texto, isto resolverá o problema de gerar mais textos após o modelo dar a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b8a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\n    You are a helpful assistant.<|end|>\\n    <|user|>\\n    Quem foi a primeira pessoa no espaço?<|end|>\\n    <|assistant|>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c206565",
   "metadata": {},
   "source": [
    "O problema da geração de novos textos sem contexto com a pergunta foi resolvido! Agora o modelo não gera mais textos desnecessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d61e7359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sim, eu entendo o português. Como posso ajudar você hoje?\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"Você entende de português?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed36339",
   "metadata": {},
   "source": [
    "### Explorando a engenheraria de prompt\n",
    "\n",
    "A engenharia de prompt tem o objetivo de gerar melhores textos para garantir uma melhor comunicação com uma inteligência artificial.\n",
    "\n",
    "Quando você estiver trabalhando com um problema específico e não estiver obtendo os resultados desejados, é sempre importante conferir se o prompt pode ser mais específico, pois as vezes é necessário enviar mais detalhes, tanto na pergunta quanto no prompt de sistema.\n",
    "\n",
    "Se mesmo com a melhoria dos prompts e testes com diferentes parâmetros (*generation_args*) o modelo não apresentar resultados satisfatórios, talvez o modelo não seja adequado para o problema. Portanto, é sempre interessante testar diferentes prompts, juntamente com modelos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e74cc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_sys_prompt(prompt, sys_prompt):\n",
    "    return f\"\"\"<|system|>\n",
    "    {sys_prompt}<|end|>\n",
    "    <|user|>\n",
    "    {prompt}<|end|>\n",
    "    <|assistant|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b674455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_sys_prompt(prompt, sys_prompt):\n",
    "    output = pipe(template_sys_prompt(prompt, sys_prompt), **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "    Você é um assistente virtual prestativo. Responda as perguntas em português.<|end|>\n",
      "    <|user|>\n",
      "    O que é IA?<|end|>\n",
      "    <|assistant|>\n",
      " A IA, ou Inteligência Artificial, é um campo da ciência da computação que se dedica ao desenvolvimento de sistemas capazes de realizar tarefas que normalmente exigiriam inteligência humana. Essas tarefas incluem aprendizado, raciocínio, percepção e resolução de problemas. A IA pode ser categorizada em diferentes tipos, como IA fraca (ou aprendizagem de máquina) e IA forte (ou inteligência generalizada). A IA é utilizada em diversos setores, como saúde, finanças, automação e entretenimento, proporcionando soluções inovadoras e eficiência.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O que é IA?\"\n",
    "sys_prompt = \"Você é um assistente virtual prestativo. Responda as perguntas em português.\"\n",
    "print(template_sys_prompt(prompt, sys_prompt))\n",
    "print(llm_sys_prompt(prompt, sys_prompt)) # Exibe a resposta do modelo em sequência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31027a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "    Você é um programador experiente. Retorne o código requisitado e forneça explicações breves se achar conveniente<|end|>\n",
      "    <|user|>\n",
      "    Gere um código em python que escreva a sequência de fibonnaci<|end|>\n",
      "    <|assistant|>\n",
      " ```python\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "    a, b = 0, 1\n",
      "\n",
      "    sequence = []\n",
      "\n",
      "    while len(sequence) < n:\n",
      "\n",
      "        sequence.append(a)\n",
      "\n",
      "        a, b = b, a + b\n",
      "\n",
      "    return sequence\n",
      "\n",
      "\n",
      "# Exemplo de uso:\n",
      "\n",
      "n = 10  # Quantidade de números da sequência de Fibonacci desejada\n",
      "\n",
      "print(fibonacci(n))\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "Esta função `fibonacci` recebe um número `n` e retorna uma lista com os primeiros `n` números da sequência de Fibonacci. A sequência começa com 0 e 1, e cada número subsequente é a soma dos dois anteriores.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Gere um código em python que escreva a sequência de fibonnaci\"\n",
    "sys_prompt = \"Você é um programador experiente. Retorne o código requisitado e forneça explicações breves se achar conveniente\"\n",
    "print(template_sys_prompt(prompt, sys_prompt))\n",
    "print(llm_sys_prompt(prompt, sys_prompt)) # Exibe a resposta do modelo em sequência"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8fdff",
   "metadata": {},
   "source": [
    "### Formato de mensagens\n",
    "\n",
    "- **Role system**: Indica o que nós estamos especificando sobre o modelo (sys_prompt).\n",
    "- **Role user**: Indica que a mensagem é do usuário.\n",
    "\n",
    "Em resumo, essa é uma maneira alternativa para obtermos o mesmo resultado que já haviamos trabalhado, porém sem a utilização do *template*. Com isso, o código fica um pouco mais limpo. É apenas uma maneira alternativa e mais fácil se comparado com a utilização do template.\n",
    "\n",
    "Quando utilizamos esse tipo de formato de mensagens (assim como com templates), nós garantimos que o modelo não vai cometer alucinações e gerar textos desnecessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c7730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A IA, ou Inteligência Artificial, é um campo da ciência da computação que se dedica ao desenvolvimento de sistemas capazes de realizar tarefas que normalmente exigiriam inteligência humana. Essas tarefas incluem aprendizado, raciocínio, percepção e resolução de problemas. A IA pode ser categorizada em diferentes tipos, como IA fraca (ou aprendizagem de máquina) e IA forte (ou inteligência generalizada), e é aplicada em diversos domínios, como saúde, finanças, automação e entretenimento.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O que é IA?\"\n",
    "msg = [\n",
    "    {\"role\": \"system\", \"content\": \"Você é um assistente virtual prestativo. Responda as perguntas em português.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "output = pipe(msg, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "534981fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_msg(prompt: str, sys_prompt: str):\n",
    "    msg = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    output = pipe(msg, **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "410f5b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. São Paulo\n",
      "\n",
      "2. Rio de Janeiro\n",
      "\n",
      "3. Brasília\n",
      "\n",
      "4. Salvador\n",
      "\n",
      "5. Fortaleza\n",
      "\n",
      "6. Recife\n",
      "\n",
      "7. Belo Horizonte\n",
      "\n",
      "8. Curitiba\n",
      "\n",
      "9. Porto Alegre\n",
      "\n",
      "10. Manaus\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Liste o nome de 10 cidades famosas do Brasil\"\n",
    "sys_prompt = \"Você é um assistente virtual prestativo. Responda as perguntas em português.\"\n",
    "\n",
    "print(llm_msg(prompt, sys_prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
