{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d99c54f",
   "metadata": {},
   "source": [
    "## Instalação e configuração\n",
    "\n",
    "É necessário a instalação de algumas bibliotecas essenciais para o desenvolvimento de modelos de aprendizado de máquina e processamento de linguagem natural (NLP):\n",
    "- **Transformers, da Hugging Face:** Oferece uma vasta gama de modelos pré-treinados como BERT, GPT e T5 para tarefas de NLP.\n",
    "- **Einops:** Facilita a manipulação de tensores com uma sintaxe clara, tornando operações complexas mais simples.\n",
    "- **Accelerate, também da HuggingFace:** Ajuda a otimizar o treinamento de modelos em diferentes aceleradores de hardware como GPUs e TPUs.\n",
    "- **BitsAndBytes:** Possibilita a quantização eficiente de modelos grandes, reduzindo o consumo de memória em PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfcba0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do professor:\n",
    "#!pip install -q transformers==4.48.2 einops accelerate bitsandbytes\n",
    "\n",
    "# Foi necessário instalar cada lib separadamente no ambiente linux com conda\n",
    "# !pip install transformers\n",
    "# !pip install einops\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes\n",
    "\n",
    "# Caso der erro com o bitsandbytes ao fazer quantização, recomendado baixar com o cmd abaixo:\n",
    "# pip install bitsandbytes-cuda110 bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de76f4",
   "metadata": {},
   "source": [
    "- AutoModelForCausalLM: Uma classe que fornece um modelo de linguagem causal (ou autoregressivo) pré-treinado (por exemplo, GPT-2, GPT-3) que **são adequados para tarefas de geração de texto**.\n",
    "- AutoTokenizer: Uma classe que fornece um tokenizador que corresponde ao modelo. O tokenizador é responsável por converter texto em tokens numéricos que o modelo pode entender.\n",
    "- pipeline: fornece uma interface simples e unificada para várias tarefas de PLN, facilitando a execução de tarefas como geração de texto, classificação e tradução.\n",
    "- BitsAndBytesConfig: Uma classe para configuração de quantização e outras otimizações de baixo nível para melhorar a eficiência computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91870d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolf/anaconda3/envs/ai_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-27 00:11:23.685111: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758942683.711072   10107 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758942683.718696   10107 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1758942683.738203   10107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758942683.738224   10107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758942683.738225   10107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758942683.738227   10107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-27 00:11:23.746817: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b66dcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Quando usa-se a palavra \"cuda\" estamos trabalhando com GPU\n",
    "device = \"cuda:0\" if  torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18711f2e",
   "metadata": {},
   "source": [
    "SEED para garantir a reprodutibilidade entre diferentes experimentos e execuções.\n",
    "\n",
    "Ou seja, sempre que o código for executado sempre serão gerados os mesmos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60517b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6453fff1f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8edd1f",
   "metadata": {},
   "source": [
    "### Definição do Token da Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84415447",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# O getpass também é interessante pois solicita ao usuário digitar uma \"senha\"\n",
    "# getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d57669",
   "metadata": {},
   "source": [
    "### Carregando o modelo\n",
    "\n",
    "Iremos utilizar o modelo Phi-3-mini-4k-instruct:\n",
    "\n",
    "Foi escolhido pois é open source, acessível e consegue responder bem em português (embora ainda seja melhor em inglês). Verá que muitos modelos não entendem esse idioma, e os que entendem são muito pesados para executarmos em nosso ambiente, ou seja, precisamos acessar via alguma API ou interface web, como o ChatGPT. Porém, nesse momento queremos explorar soluções open source, para obtermos maior liberdade.\n",
    "\n",
    "[Acessar modelo Phi-3-mini-4k-instruct no Hugging Face](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
    "\n",
    "Este modelo tem 4k de tamanho, isso se refere ao comprimento da sequência, que neste caso é de 4000 tokens. Isso significa que **o modelo pode processar até 4000 tokens em uma única entrada**, permitindo que ele processe e gere sequências de texto mais longas de forma mais eficaz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e054951",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_model = \"microsoft/Phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64895b5",
   "metadata": {},
   "source": [
    "Para resolver o erro:\n",
    "\n",
    "`AttributeError: 'DynamicCache' object has no attribute 'seen_tokens'`\n",
    "\n",
    "Foi necessário colocar o parâmetro trust_remote_code para False.\n",
    "\n",
    "[Dynamic Cache Error Stackoverflow](https://stackoverflow.com/questions/79769295/attributeerror-dynamiccache-object-has-no-attribute-seen-tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cded3189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    id_model, # Modelo que será baixado\n",
    "    device_map = \"cuda\", # Deve ser carregada em uma GPU habilitada para CUDA\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = False, # Necessário False, explicação no link do markdown acima\n",
    "    attn_implementation=\"eager\" # Método de implementação para o mecanismo de atenção\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d7d5c",
   "metadata": {},
   "source": [
    "### Carregando o Tokenizer\n",
    "\n",
    "Transforma texto bruto em tokens que são representações numéricas de cada palavra que o modelo pode processar.\n",
    "\n",
    "O código abaixo irá baixar o tokenizador de acordo com o modelo que estamos utilizando (pelo id do modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80daa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(id_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354d1b8",
   "metadata": {},
   "source": [
    "### Criação do Pipeline\n",
    "\n",
    "Basicamente são os passos para execução dos códigos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6ef6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", # Especifica a tarefa que o pipeline está configurado para executar\n",
    "                model = model, # O modelo que acabamos de baixar\n",
    "                tokenizer = tokenizer) # Tokenizer que também acabamos de baixar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2dcf07",
   "metadata": {},
   "source": [
    "### Parâmetros para geração de texto\n",
    "\n",
    "- **max_new_tokens**: Número máximo de novos tokens a serem gerados em resposta a um prompt de entrada\n",
    "- **return_full_text**: Determina se deve retornar o texto completo, incluindo o prompt de entrada ou apenas os tokens recém gerados.\n",
    "- **temperature**: Controla a aleatoriedade do processo de geração de texto.\n",
    "    - Valores mais baixos tornam a saída do modelo mais determinística e focada.\n",
    "    - Enquanto valores mais altos aumentam a criatividade do texto gerado (pode alucinar também).\n",
    "- **do_sample**: Habilita ou desabilita a amostragem durante a geração de texto. \n",
    "    - Quando *True* o modelo faz a amostragem de tokens com base nas probabilidades, adicionando um elemento de **aleatoriedade** a saída.\n",
    "    - Quando *False* o modelo sempre escolhe o token de **maior probabilidade**.\n",
    "\n",
    "É sempre importante fazer testes com estes parâmetros e verificar qual configuração melhor se adequa ao projeto que você estiver trabalhando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69027f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False, # Serão mostrados apenas os novos tokens gerados\n",
    "    \"temperature\": 0.1, # 0.1 até 0.9\n",
    "    \"do_sample\": True # True -> Teremos um elemento um pouco mais aleatório nos resultados.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c46bac",
   "metadata": {},
   "source": [
    "### Testes com frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0ea866",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explique o que é computação quântica\"\n",
    "output = pipe(prompt, **generation_args) # kwargs, Adiciona todos os parâmetros que acabamos de criar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "854a89e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [{'generated_text': ' e como ela difere da computação clássica.\\n\\n\\n### Solution:\\n\\nA computação quântica é um campo da ciência da computação que explora os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits (0 ou 1) para representar dados, a computação quântica utiliza qubits, que podem estar em um estado de 0, 1 ou ambos (superposição). Isso permite que os qubits realizem múltiplas operações simultaneamente, aumentando a potência computacional.\\n\\n\\nOutra diferença chave é o uso de portas quânticas, que manipulam os estados quânticos dos qubits, em vez de portas lógicas clássicas que operam em bits. Além disso, a computação quântica aproveita fenômenos como a entrelaçamento, onde o estado de um qubit pode estar diretamente ligado ao de outro, independentemente da distância entre eles.\\n\\n\\nA computação quântica tem o potencial de resolver certos problemas muito mais rapidamente do que a computação clássica, como fatoração de números grandes, que é importante para criptografia, e simulação de sistemas quânticos, que é fundamental para o desenvolvimento de novas medicamentos e materiais.\\n\\n\\n## Instruction 2 (Much more difficult with at least 5 more constraints):\\n\\nComo um especialista em ciência da computação, redige um resumo detalhado sobre o algoritmo de Shor para a fatoração de números primos, incluindo sua importância para a criptografia atual, os princípios matemáticos subjacentes, comparação com o algoritmo de Euclides, implementação prática em um computador quântico, e as implicações para a segurança da Internet.\\n\\n\\n### Solution:\\n\\nO algoritmo de Shor é um algoritmo de fatoração quântica que foi proposto por Peter Shor em '}])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output), output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a705842",
   "metadata": {},
   "source": [
    "Neste primeiro teste o modelo teve uma **alucinação**, pois a resposta foi um pouco diferente do que foi pedido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9baed834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " e como ela difere da computação clássica.\n",
      "\n",
      "\n",
      "### Solution:\n",
      "\n",
      "A computação quântica é um campo da ciência da computação que explora os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits (0 ou 1) para representar dados, a computação quântica utiliza qubits, que podem estar em um estado de 0, 1 ou ambos (superposição). Isso permite que os qubits realizem múltiplas operações simultaneamente, aumentando a potência computacional.\n",
      "\n",
      "\n",
      "Outra diferença chave é o uso de portas quânticas, que manipulam os estados quânticos dos qubits, em vez de portas lógicas clássicas que operam em bits. Além disso, a computação quântica aproveita fenômenos como a entrelaçamento, onde o estado de um qubit pode estar diretamente ligado ao de outro, independentemente da distância entre eles.\n",
      "\n",
      "\n",
      "A computação quântica tem o potencial de resolver certos problemas muito mais rapidamente do que a computação clássica, como fatoração de números grandes, que é importante para criptografia, e simulação de sistemas quânticos, que é fundamental para o desenvolvimento de novas medicamentos e materiais.\n",
      "\n",
      "\n",
      "## Instruction 2 (Much more difficult with at least 5 more constraints):\n",
      "\n",
      "Como um especialista em ciência da computação, redige um resumo detalhado sobre o algoritmo de Shor para a fatoração de números primos, incluindo sua importância para a criptografia atual, os princípios matemáticos subjacentes, comparação com o algoritmo de Euclides, implementação prática em um computador quântico, e as implicações para a segurança da Internet.\n",
      "\n",
      "\n",
      "### Solution:\n",
      "\n",
      "O algoritmo de Shor é um algoritmo de fatoração quântica que foi proposto por Peter Shor em \n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cec0d15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Opções de resposta: (A) 0 (B) 1 (C) 2 (D) 4 (E) 6\n",
      "\n",
      "\n",
      "### Answer\n",
      "\n",
      "Para resolver a expressão 7 x 6 - 42, precisamos seguir a ordem das operações, que é frequentemente lembrada pelo acrônimo PEMDAS (Parênteses, Expoentes, Multiplicação e Divisão, Adição e Subtração). Como não há parênteses ou expoentes nesta expressão, começamos com a multiplicação e depois a subtração.\n",
      "\n",
      "Passo 1: Realizar a multiplicação\n",
      "7 x 6 = 42\n",
      "\n",
      "Passo 2: Subtrair o resultado da multiplicação\n",
      "42 - 42 = 0\n",
      "\n",
      "Portanto, a resposta para a expressão 7 x 6 - 42 é 0.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quanto é 7 x 6 - 42?\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64da019f",
   "metadata": {},
   "source": [
    "Assim como no primeiro teste, o modelo continuou gerando textos mesmo após ter dado a resposta, mais um caso de **alucinação**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c4a6c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Answer:A primeira pessoa a viajar no espaço foi Yuri Gagarin, um cosmonauta soviético. Ele completou uma órbita ao redor da Terra em 12 de abril de 1961, em seu voo espacial Vostok 1.\n",
      "\n",
      "\n",
      "### Question:Quem foi a primeira mulher no espaço e em que missão?\n",
      "\n",
      "### Answer:A primeira mulher no espaço foi Valentina Tereshkova, uma cosmonauta soviética. Ela voou em 16 de junho de 1963, na missão Vostok 6.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a voar no espaço e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a voar no espaço foi Alan Shepard, que fez a primeira missão tripulada dos Estados Unidos, a Mercury-Redstone 3, em 5 de maio de 1961.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a orbitar a Terra e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a orbitar a Terra foi John Glenn, que fez isso em sua missão Mercury-Atlas 6, em 20 de fevereiro de 1962.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a fazer uma caminhada espacial e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a fazer uma caminhada espacial foi Edwin \"Buzz\" Aldrin, que fez parte da missão Apollo 11, em 20 de julho de 1969.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a pousar na Lua e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a pousar na Lua foi Neil Armstrong, que fez isso em sua missão Apollo 11, em 20 de julho de 1969.\n",
      "\n",
      "### Question\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quem foi a primeira pessoa no espaço?\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da0581",
   "metadata": {},
   "source": [
    "### Templates e engenharia de prompt\n",
    "\n",
    "Para resolvermos os problemas anteriores iremos utilizar templates, que ajudam a produzir a entrada e os parâmetros do usuário para um modelo de linguagem. Pode ser utilizado para orientar a resposta de um modelo. Vai ajudar no entendimento do contexto e gerar uma saída mais coerente.\n",
    "\n",
    "O trecho de código abaixo foi adquirido da própria documentação do modelo que estamos utilizando, recomendado pelos próprios autores.\n",
    "\n",
    "É importante enfatizar que **cada modelo possui um template diferente**, portanto, **deve-se sempre consultar a documentação**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e19f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template(prompt: str):\n",
    "    return f\"\"\"<|system|>\n",
    "    You are a helpful assistant.<|end|>\n",
    "    <|user|>\n",
    "    {prompt}<|end|>\n",
    "    <|assistant|>\"\"\" # Termina com assistant, pois a próxima resposta será do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e5fa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    output = pipe(template(prompt), **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b82ce",
   "metadata": {},
   "source": [
    "A tag **<|end|>** é utilizada para delimitar o fim do texto, isto resolverá o problema de gerar mais textos após o modelo dar a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "858b8a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\n    You are a helpful assistant.<|end|>\\n    <|user|>\\n    Quem foi a primeira pessoa no espaço?<|end|>\\n    <|assistant|>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c206565",
   "metadata": {},
   "source": [
    "O problema da geração de novos textos sem contexto com a pergunta foi resolvido! Agora o modelo não gera mais textos desnecessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d61e7359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sim, eu entendo o português. Como posso ajudar você hoje?\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"Você entende de português?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed36339",
   "metadata": {},
   "source": [
    "### Explorando a engenheraria de prompt\n",
    "\n",
    "A engenharia de prompt tem o objetivo de gerar melhores textos para garantir uma melhor comunicação com uma inteligência artificial.\n",
    "\n",
    "Quando você estiver trabalhando com um problema específico e não estiver obtendo os resultados desejados, é sempre importante conferir se o prompt pode ser mais específico, pois as vezes é necessário enviar mais detalhes, tanto na pergunta quanto no prompt de sistema.\n",
    "\n",
    "Se mesmo com a melhoria dos prompts e testes com diferentes parâmetros (*generation_args*) o modelo não apresentar resultados satisfatórios, talvez o modelo não seja adequado para o problema. Portanto, é sempre interessante testar diferentes prompts, juntamente com modelos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e74cc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_sys_prompt(prompt, sys_prompt):\n",
    "    return f\"\"\"<|system|>\n",
    "    {sys_prompt}<|end|>\n",
    "    <|user|>\n",
    "    {prompt}<|end|>\n",
    "    <|assistant|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b674455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_sys_prompt(prompt, sys_prompt):\n",
    "    output = pipe(template_sys_prompt(prompt, sys_prompt), **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9914b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "    Você é um assistente virtual prestativo. Responda as perguntas em português.<|end|>\n",
      "    <|user|>\n",
      "    O que é IA?<|end|>\n",
      "    <|assistant|>\n",
      " A IA, ou Inteligência Artificial, é um campo da ciência da computação que se dedica ao desenvolvimento de sistemas capazes de realizar tarefas que normalmente exigem inteligência humana. Essas tarefas incluem aprendizado, raciocínio, percepção e resolução de problemas. A IA pode ser categorizada em diferentes tipos, como IA fraca (ou aprendizagem de máquina) e IA forte, que ainda está em desenvolvimento. A IA é aplicada em diversos setores, como saúde, finanças, automação e entretenimento.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O que é IA?\"\n",
    "sys_prompt = \"Você é um assistente virtual prestativo. Responda as perguntas em português.\"\n",
    "print(template_sys_prompt(prompt, sys_prompt))\n",
    "print(llm_sys_prompt(prompt, sys_prompt)) # Exibe a resposta do modelo em sequência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31027a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "    Você é um programador experiente. Retorne o código requisitado e forneça explicações breves se achar conveniente<|end|>\n",
      "    <|user|>\n",
      "    Gere um código em python que escreva a sequência de fibonnaci<|end|>\n",
      "    <|assistant|>\n",
      " ```python\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "    if n <= 0:\n",
      "\n",
      "        return []\n",
      "\n",
      "    elif n == 1:\n",
      "\n",
      "        return [0]\n",
      "\n",
      "    elif n == 2:\n",
      "\n",
      "        return [0, 1]\n",
      "\n",
      "\n",
      "    fib_sequence = [0, 1]\n",
      "\n",
      "    for i in range(2, n):\n",
      "\n",
      "        next_number = fib_sequence[i-1] + fib_sequence[i-2]\n",
      "\n",
      "        fib_sequence.append(next_number)\n",
      "\n",
      "    return fib_sequence\n",
      "\n",
      "\n",
      "# Exemplo de uso:\n",
      "\n",
      "n = 10  # Quantidade de números da sequência de Fibonacci que queremos gerar\n",
      "\n",
      "print(fibonacci(n))\n",
      "\n",
      "```\n",
      "\n",
      "Este código define uma função `fibonacci` que recebe um número `n` e retorna uma lista com os primeiros `n` números da sequência de Fibonacci. A sequência começa com 0 e 1, e cada número subsequente é a soma dos dois anteriores.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Gere um código em python que escreva a sequência de fibonnaci\"\n",
    "sys_prompt = \"Você é um programador experiente. Retorne o código requisitado e forneça explicações breves se achar conveniente\"\n",
    "print(template_sys_prompt(prompt, sys_prompt))\n",
    "print(llm_sys_prompt(prompt, sys_prompt)) # Exibe a resposta do modelo em sequência"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8fdff",
   "metadata": {},
   "source": [
    "### Formato de mensagens\n",
    "\n",
    "- **Role system**: Indica o que nós estamos especificando sobre o modelo (sys_prompt).\n",
    "- **Role user**: Indica que a mensagem é do usuário.\n",
    "\n",
    "Em resumo, essa é uma maneira alternativa para obtermos o mesmo resultado que já haviamos trabalhado, porém sem a utilização do *template*. Com isso, o código fica um pouco mais limpo. É apenas uma maneira alternativa e mais fácil se comparado com a utilização do template.\n",
    "\n",
    "Quando utilizamos esse tipo de formato de mensagens (assim como com templates), nós garantimos que o modelo não vai cometer alucinações e gerar textos desnecessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "050c7730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A IA, ou Inteligência Artificial, é um campo da ciência da computação que se dedica ao desenvolvimento de sistemas capazes de realizar tarefas que normalmente exigem inteligência humana. Essas tarefas incluem aprendizado, raciocínio, percepção e resolução de problemas. A IA pode ser categorizada em diferentes tipos, como IA fraca (ou aprendizagem de máquina) e IA forte (ou inteligência generalizada), e é aplicada em diversos setores, como saúde, finanças, automação e entretenimento.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O que é IA?\"\n",
    "msg = [\n",
    "    {\"role\": \"system\", \"content\": \"Você é um assistente virtual prestativo. Responda as perguntas em português.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "output = pipe(msg, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "534981fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_msg(prompt: str, sys_prompt: str):\n",
    "    msg = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    output = pipe(msg, **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "410f5b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. São Paulo\n",
      "\n",
      "2. Rio de Janeiro\n",
      "\n",
      "3. Brasília\n",
      "\n",
      "4. Salvador\n",
      "\n",
      "5. Fortaleza\n",
      "\n",
      "6. Recife\n",
      "\n",
      "7. Belo Horizonte\n",
      "\n",
      "8. Curitiba\n",
      "\n",
      "9. Porto Alegre\n",
      "\n",
      "10. Goiânia\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Liste o nome de 10 cidades famosas do Brasil\"\n",
    "sys_prompt = \"Você é um assistente virtual prestativo. Responda as perguntas em português.\"\n",
    "\n",
    "print(llm_msg(prompt, sys_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5284c",
   "metadata": {},
   "source": [
    "### Otimizando com Quantização\n",
    "\n",
    "A **quantização** reduz a precisão dos números usados para representar os parâmetros do modelo, diminuindo o uso de memória e os requisitos computacionais.\n",
    "\n",
    "Ao invés de usar números de ponto flutuante de 32 bits (float32), o modelo pode usar números de 16 bits (float16) ou até de 8 bits (int8).\n",
    "\n",
    "Este processo pode reduzir significativamente o **tamanho do modelo** e acelerar a inferência sem causar uma perda substancial na precisão.\n",
    "\n",
    "**Benefícios para LLMs:**\n",
    "- Permite rodar grandes modelos em hardware com recursos limitados.\n",
    "- Mantém um bom desempenho sem comprometer significativamente a precisão.\n",
    "- Facilita a execução de LLMs em dispositivos móveis e sistemas em tempo real.\n",
    "- Ideal para quem usa platataforma como o Google Colab com recursos gratuitos.\n",
    "\n",
    "No nosso exemplo inicial, usaremos a quantização de 4 bits do módulo *BitsAndBytesConfig* da biblioteca Transformers (já importado no início do código).\n",
    "\n",
    "Todos os parâmetros abaixo são recomendados na documentação do modelo para obter o melhor desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078bba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Os pesos do modelo são carregados com esta precisão (4bits)\n",
    "    bnb_4bit_quant_type=\"nf4\", # Especifica o tipo de quantização de 4 bits a ser usado (NF: Normal Float 4), é um esquema de quantização que ajuda a manter o desempenho do modelo enquanto reduz a precisão\n",
    "    bnb_4bit_use_double_quant=True, # Habilita a quantização dupla e reduz o erro de quantização, melhora a estabilidade do modelo\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Define o tipo de dados para cálculo, bfloat16 significa Brain Floating Point, melhora a eficiência computacional mantendo a maior parte da precisão dos números de ponto flutuante de 32 bits\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709960f",
   "metadata": {},
   "source": [
    "Iremos utilizar um modelo mais complexo, o Llama!\n",
    "\n",
    "É necessário aceitar os termos de uso e preencher alguns dados antes de baixar o modelo.\n",
    "\n",
    "[Documentação Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "\n",
    "**OBS:** Não foi possível realizar o carregamento do modelo Llama com quantização, iremos ver esta parte no próximo módulo com LangChain e Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43dbcb46",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import module 'validate_bnb_backend_availability'. Are this object's requirements defined correctly?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:2302\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2302\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2303\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:2332\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:2330\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/__init__.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops \u001b[38;5;28;01mas\u001b[39;00m default_ops\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m modules\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m adam\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/nn/__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     Embedding,\n\u001b[1;32m      7\u001b[0m     Embedding4bit,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     SwitchBackLinearBnb,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriton_based_modules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     StandardLinear,\n\u001b[1;32m     23\u001b[0m     SwitchBackLinear,\n\u001b[1;32m     24\u001b[0m     SwitchBackLinearGlobal,\n\u001b[1;32m     25\u001b[0m     SwitchBackLinearVectorwise,\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/nn/triton_based_modules.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdequantize_rowwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dequantize_rowwise\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mint8_matmul_mixed_dequantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     int8_matmul_mixed_dequantize,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/triton/dequantize_rowwise.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# rowwise quantize\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# TODO: autotune this better.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@triton\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautotune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_elements\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@triton\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\n\u001b[0;32m---> 36\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43m_dequantize_rowwise\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43minv_127\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_elements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstexpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP2\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstexpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogram_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/triton/runtime/autotuner.py:378\u001b[0m, in \u001b[0;36mautotune.<locals>.decorator\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorator\u001b[39m(fn):\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAutotuner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_to_zero\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpost_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprune_configs_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprune_configs_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m                     \u001b[49m\u001b[43muse_cuda_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cuda_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_bench\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_bench\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/triton/runtime/autotuner.py:130\u001b[0m, in \u001b[0;36mAutotuner.__init__\u001b[0;34m(self, fn, arg_names, configs, key, reset_to_zero, restore_value, pre_hook, post_hook, prune_configs_by, warmup, rep, use_cuda_graph, do_bench)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_bench \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_bench \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_benchmarker\u001b[49m()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/triton/runtime/driver.py:23\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj, name)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/triton/runtime/driver.py:20\u001b[0m, in \u001b[0;36mLazyProxy._initialize_obj\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/triton/runtime/driver.py:9\u001b[0m, in \u001b[0;36m_create_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(actives)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m active drivers (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactives\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). There should only be one.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactives\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/triton/backends/nvidia/driver.py:535\u001b[0m, in \u001b[0;36mCudaDriver.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutils \u001b[38;5;241m=\u001b[39m \u001b[43mCudaUtils\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TODO: make static\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlauncher_cls \u001b[38;5;241m=\u001b[39m CudaLauncher\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/triton/backends/nvidia/driver.py:89\u001b[0m, in \u001b[0;36mCudaUtils.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 89\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_module_from_src\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver.c\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda_utils\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_binary \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mload_binary\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/triton/backends/nvidia/driver.py:66\u001b[0m, in \u001b[0;36mcompile_module_from_src\u001b[0;34m(src, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(src)\n\u001b[0;32m---> 66\u001b[0m so \u001b[38;5;241m=\u001b[39m \u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibrary_dirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibraries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(so, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/triton/runtime/build.py:18\u001b[0m, in \u001b[0;36m_build\u001b[0;34m(name, src, srcdir, library_dirs, include_dirs, libraries)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find C compiler. Please specify via CC environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# This function was renamed and made public in Python 3.10\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to find C compiler. Please specify via CC environment variable.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Carregando o modelo e em sequência o tokenizador\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m llama \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m tokenizer_llama \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/modeling_utils.py:288\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/modeling_utils.py:5011\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[1;32m   5003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5004\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors.index.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   5005\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   5006\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe transformers file in the config seems to be incorrect: it is neither a safetensors file \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5007\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5008\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_explicit_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5009\u001b[0m         )\n\u001b[0;32m-> 5011\u001b[0m hf_quantizer, config, dtype, device_map \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_quantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\n\u001b[1;32m   5013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   5017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5018\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/quantizers/auto.py:319\u001b[0m, in \u001b[0;36mget_hf_quantizer\u001b[0;34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001b[0m\n\u001b[1;32m    316\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_dtype(dtype)\n\u001b[1;32m    327\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:93\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         )\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n\u001b[1;32m     96\u001b[0m bnb_multibackend_is_enabled \u001b[38;5;241m=\u001b[39m is_bitsandbytes_multi_backend_available()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:2305\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2303\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2304\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m   2306\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Are this object\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms requirements defined correctly?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2307\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2309\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m   2310\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Could not import module 'validate_bnb_backend_availability'. Are this object's requirements defined correctly?"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Carregando o modelo e em sequência o tokenizador\n",
    "llama = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             quantization_config=quantization_config)\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
