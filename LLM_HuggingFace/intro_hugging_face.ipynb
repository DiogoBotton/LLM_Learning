{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d99c54f",
   "metadata": {},
   "source": [
    "## Instalação e configuração\n",
    "\n",
    "É necessário a instalação de algumas bibliotecas essenciais para o desenvolvimento de modelos de aprendizado de máquina e processamento de linguagem natural (NLP):\n",
    "- **Transformers, da Hugging Face:** Oferece uma vasta gama de modelos pré-treinados como BERT, GPT e T5 para tarefas de NLP.\n",
    "- **Einops:** Facilita a manipulação de tensores com uma sintaxe clara, tornando operações complexas mais simples.\n",
    "- **Accelerate, também da HuggingFace:** Ajuda a otimizar o treinamento de modelos em diferentes aceleradores de hardware como GPUs e TPUs.\n",
    "- **BitsAndBytes:** Possibilita a quantização eficiente de modelos grandes, reduzindo o consumo de memória em PyTorch.\n",
    "\n",
    "### **Dica para liberar espaço após uso:**\n",
    "\n",
    "Para deinstalar o cache dos modelos, baixe o cli do hugging face:\n",
    "```bash\n",
    "pip install -U \"huggingface_hub[cli]\"\n",
    "```\n",
    "\n",
    "Para deletar, rode o seguinte comando e selecione qual modelo quer remover:\n",
    "```bash\n",
    "huggingface-cli delete-cache\n",
    "```\n",
    "\n",
    "Fonte: [How To Safely Delete a HuggingFace Model From the Cache](https://medium.com/@airabbitX/how-to-safely-delete-a-hugging-face-model-from-the-cache-7d9dcd9a7036)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfcba0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do professor:\n",
    "#!pip install -q transformers==4.48.2 einops accelerate bitsandbytes\n",
    "\n",
    "# Foi necessário instalar cada lib separadamente no ambiente linux com conda\n",
    "# !pip install transformers\n",
    "# !pip install einops\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes\n",
    "\n",
    "# Caso der erro com o bitsandbytes ao fazer quantização, recomendado baixar com o cmd abaixo:\n",
    "# pip install bitsandbytes-cuda110 bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de76f4",
   "metadata": {},
   "source": [
    "- AutoModelForCausalLM: Uma classe que fornece um modelo de linguagem causal (ou autoregressivo) pré-treinado (por exemplo, GPT-2, GPT-3) que **são adequados para tarefas de geração de texto**.\n",
    "- AutoTokenizer: Uma classe que fornece um tokenizador que corresponde ao modelo. O tokenizador é responsável por converter texto em tokens numéricos que o modelo pode entender.\n",
    "- pipeline: fornece uma interface simples e unificada para várias tarefas de PLN, facilitando a execução de tarefas como geração de texto, classificação e tradução.\n",
    "- BitsAndBytesConfig: Uma classe para configuração de quantização e outras otimizações de baixo nível para melhorar a eficiência computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91870d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolf/anaconda3/envs/ai_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-27 12:05:24.456508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758985524.550769    7970 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758985524.576641    7970 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1758985524.773541    7970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758985524.773617    7970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758985524.773619    7970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758985524.773620    7970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-27 12:05:24.795402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b66dcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Quando usa-se a palavra \"cuda\" estamos trabalhando com GPU\n",
    "device = \"cuda:0\" if  torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d897f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18711f2e",
   "metadata": {},
   "source": [
    "SEED para garantir a reprodutibilidade entre diferentes experimentos e execuções.\n",
    "\n",
    "Ou seja, sempre que o código for executado sempre serão gerados os mesmos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60517b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff8c834b970>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8edd1f",
   "metadata": {},
   "source": [
    "### Definição do Token da Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84415447",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# O getpass também é interessante pois solicita ao usuário digitar uma \"senha\"\n",
    "# getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d57669",
   "metadata": {},
   "source": [
    "### Carregando o modelo\n",
    "\n",
    "Iremos utilizar o modelo Phi-3-mini-4k-instruct:\n",
    "\n",
    "Foi escolhido pois é open source, acessível e consegue responder bem em português (embora ainda seja melhor em inglês). Verá que muitos modelos não entendem esse idioma, e os que entendem são muito pesados para executarmos em nosso ambiente, ou seja, precisamos acessar via alguma API ou interface web, como o ChatGPT. Porém, nesse momento queremos explorar soluções open source, para obtermos maior liberdade.\n",
    "\n",
    "[Acessar modelo Phi-3-mini-4k-instruct no Hugging Face](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
    "\n",
    "Este modelo tem 4k de tamanho, isso se refere ao comprimento da sequência, que neste caso é de 4000 tokens. Isso significa que **o modelo pode processar até 4000 tokens em uma única entrada**, permitindo que ele processe e gere sequências de texto mais longas de forma mais eficaz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e054951",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_model = \"microsoft/Phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64895b5",
   "metadata": {},
   "source": [
    "Para resolver o erro:\n",
    "\n",
    "`AttributeError: 'DynamicCache' object has no attribute 'seen_tokens'`\n",
    "\n",
    "Foi necessário colocar o parâmetro trust_remote_code para False.\n",
    "\n",
    "[Dynamic Cache Error Stackoverflow](https://stackoverflow.com/questions/79769295/attributeerror-dynamiccache-object-has-no-attribute-seen-tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cded3189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.53s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    id_model, # Modelo que será baixado\n",
    "    device_map = \"cuda\", # Deve ser carregada em uma GPU habilitada para CUDA\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = False, # Necessário False, explicação no link do markdown acima\n",
    "    attn_implementation=\"eager\" # Método de implementação para o mecanismo de atenção\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d7d5c",
   "metadata": {},
   "source": [
    "### Carregando o Tokenizer\n",
    "\n",
    "Transforma texto bruto em tokens que são representações numéricas de cada palavra que o modelo pode processar.\n",
    "\n",
    "O código abaixo irá baixar o tokenizador de acordo com o modelo que estamos utilizando (pelo id do modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a80daa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(id_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354d1b8",
   "metadata": {},
   "source": [
    "### Criação do Pipeline\n",
    "\n",
    "Basicamente são os passos para execução dos códigos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d6ef6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", # Especifica a tarefa que o pipeline está configurado para executar\n",
    "                model = model, # O modelo que acabamos de baixar\n",
    "                tokenizer = tokenizer) # Tokenizer que também acabamos de baixar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2dcf07",
   "metadata": {},
   "source": [
    "### Parâmetros para geração de texto\n",
    "\n",
    "- **max_new_tokens**: Número máximo de novos tokens a serem gerados em resposta a um prompt de entrada\n",
    "- **return_full_text**: Determina se deve retornar o texto completo, incluindo o prompt de entrada ou apenas os tokens recém gerados.\n",
    "- **temperature**: Controla a aleatoriedade do processo de geração de texto.\n",
    "    - Valores mais baixos tornam a saída do modelo mais determinística e focada.\n",
    "    - Enquanto valores mais altos aumentam a criatividade do texto gerado (pode alucinar também).\n",
    "- **do_sample**: Habilita ou desabilita a amostragem durante a geração de texto. \n",
    "    - Quando *True* o modelo faz a amostragem de tokens com base nas probabilidades, adicionando um elemento de **aleatoriedade** a saída.\n",
    "    - Quando *False* o modelo sempre escolhe o token de **maior probabilidade**.\n",
    "\n",
    "É sempre importante fazer testes com estes parâmetros e verificar qual configuração melhor se adequa ao projeto que você estiver trabalhando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69027f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False, # Serão mostrados apenas os novos tokens gerados\n",
    "    \"temperature\": 0.1, # 0.1 até 0.9\n",
    "    \"do_sample\": True # True -> Teremos um elemento um pouco mais aleatório nos resultados.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c46bac",
   "metadata": {},
   "source": [
    "### Testes com frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe0ea866",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explique o que é computação quântica\"\n",
    "output = pipe(prompt, **generation_args) # kwargs, Adiciona todos os parâmetros que acabamos de criar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "854a89e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [{'generated_text': ' e como ela difere da computação clássica.\\n\\n\\n### Solution:\\n\\nA computação quântica é um campo da ciência da computação que explora os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits (0 ou 1) para representar dados, a computação quântica utiliza qubits, que podem estar em um estado de 0, 1 ou ambos (superposição). Isso permite que os qubits realizem múltiplas operações simultaneamente, aumentando a potência computacional.\\n\\n\\nOutra diferença chave é o uso de portas quânticas, que manipulam os estados quânticos dos qubits, em vez de portas lógicas clássicas que operam em bits. Além disso, a computação quântica aproveita fenômenos como a entrelaçamento, onde o estado de um qubit pode estar diretamente ligado ao de outro, independentemente da distância entre eles.\\n\\n\\nA computação quântica tem o potencial de resolver certos problemas muito mais rapidamente do que a computação clássica, como fatoração de números grandes, que é importante para criptografia, e simulação de sistemas quânticos, que é fundamental para o desenvolvimento de novas medicamentos e materiais.\\n\\n\\n## Instruction 2 (Much more difficult with at least 5 more constraints):\\n\\nComo um especialista em ciência da computação, redige um resumo detalhado sobre o algoritmo de Shor para a fatoração de números primos, incluindo sua importância para a criptografia atual, os princípios matemáticos subjacentes, comparação com o algoritmo de Euclides, implementação prática em um computador quântico, e as implicações para a segurança da Internet.\\n\\n\\n### Solution:\\n\\nO algoritmo de Shor é um algoritmo de fatoração quântica que foi proposto por Peter Shor em '}])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output), output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a705842",
   "metadata": {},
   "source": [
    "Neste primeiro teste o modelo teve uma **alucinação**, pois a resposta foi um pouco diferente do que foi pedido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9baed834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " e como ela difere da computação clássica.\n",
      "\n",
      "\n",
      "### Solution:\n",
      "\n",
      "A computação quântica é um campo da ciência da computação que explora os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits (0 ou 1) para representar dados, a computação quântica utiliza qubits, que podem estar em um estado de 0, 1 ou ambos (superposição). Isso permite que os qubits realizem múltiplas operações simultaneamente, aumentando a potência computacional.\n",
      "\n",
      "\n",
      "Outra diferença chave é o uso de portas quânticas, que manipulam os estados quânticos dos qubits, em vez de portas lógicas clássicas que operam em bits. Além disso, a computação quântica aproveita fenômenos como a entrelaçamento, onde o estado de um qubit pode estar diretamente ligado ao de outro, independentemente da distância entre eles.\n",
      "\n",
      "\n",
      "A computação quântica tem o potencial de resolver certos problemas muito mais rapidamente do que a computação clássica, como fatoração de números grandes, que é importante para criptografia, e simulação de sistemas quânticos, que é fundamental para o desenvolvimento de novas medicamentos e materiais.\n",
      "\n",
      "\n",
      "## Instruction 2 (Much more difficult with at least 5 more constraints):\n",
      "\n",
      "Como um especialista em ciência da computação, redige um resumo detalhado sobre o algoritmo de Shor para a fatoração de números primos, incluindo sua importância para a criptografia atual, os princípios matemáticos subjacentes, comparação com o algoritmo de Euclides, implementação prática em um computador quântico, e as implicações para a segurança da Internet.\n",
      "\n",
      "\n",
      "### Solution:\n",
      "\n",
      "O algoritmo de Shor é um algoritmo de fatoração quântica que foi proposto por Peter Shor em \n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cec0d15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Opções de resposta: (A) 0 (B) 1 (C) 2 (D) 4 (E) 6\n",
      "\n",
      "\n",
      "### Answer\n",
      "\n",
      "Para resolver a expressão 7 x 6 - 42, precisamos seguir a ordem das operações, que é frequentemente lembrada pelo acrônimo PEMDAS (Parênteses, Expoentes, Multiplicação e Divisão, Adição e Subtração). Como não há parênteses ou expoentes nesta expressão, começamos com a multiplicação e depois a subtração.\n",
      "\n",
      "Passo 1: Realizar a multiplicação\n",
      "7 x 6 = 42\n",
      "\n",
      "Passo 2: Subtrair o resultado da multiplicação\n",
      "42 - 42 = 0\n",
      "\n",
      "Portanto, a resposta para a expressão 7 x 6 - 42 é 0.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quanto é 7 x 6 - 42?\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64da019f",
   "metadata": {},
   "source": [
    "Assim como no primeiro teste, o modelo continuou gerando textos mesmo após ter dado a resposta, mais um caso de **alucinação**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c4a6c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Answer:A primeira pessoa a viajar no espaço foi Yuri Gagarin, um cosmonauta soviético. Ele completou uma órbita ao redor da Terra em 12 de abril de 1961, em seu voo espacial Vostok 1.\n",
      "\n",
      "\n",
      "### Question:Quem foi a primeira mulher no espaço e em que missão?\n",
      "\n",
      "### Answer:A primeira mulher no espaço foi Valentina Tereshkova, uma cosmonauta soviética. Ela voou em 16 de junho de 1963, na missão Vostok 6.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a voar no espaço e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a voar no espaço foi Alan Shepard, que fez a primeira missão tripulada dos Estados Unidos, a Mercury-Redstone 3, em 5 de maio de 1961.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a orbitar a Terra e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a orbitar a Terra foi John Glenn, que fez isso em sua missão Mercury-Atlas 6, em 20 de fevereiro de 1962.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a fazer uma caminhada espacial e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a fazer uma caminhada espacial foi Edwin \"Buzz\" Aldrin, que fez parte da missão Apollo 11, em 20 de julho de 1969.\n",
      "\n",
      "\n",
      "### Question:Quem foi o primeiro astronauta americano a pousar na Lua e em que missão?\n",
      "\n",
      "### Answer:O primeiro astronauta americano a pousar na Lua foi Neil Armstrong, que fez isso em sua missão Apollo 11, em 20 de julho de 1969.\n",
      "\n",
      "### Question\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quem foi a primeira pessoa no espaço?\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da0581",
   "metadata": {},
   "source": [
    "### Templates e engenharia de prompt\n",
    "\n",
    "Para resolvermos os problemas anteriores iremos utilizar templates, que ajudam a produzir a entrada e os parâmetros do usuário para um modelo de linguagem. Pode ser utilizado para orientar a resposta de um modelo. Vai ajudar no entendimento do contexto e gerar uma saída mais coerente.\n",
    "\n",
    "O trecho de código abaixo foi adquirido da própria documentação do modelo que estamos utilizando, recomendado pelos próprios autores.\n",
    "\n",
    "É importante enfatizar que **cada modelo possui um template diferente**, portanto, **deve-se sempre consultar a documentação**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e19f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template(prompt: str):\n",
    "    return f\"\"\"<|system|>\n",
    "    You are a helpful assistant.<|end|>\n",
    "    <|user|>\n",
    "    {prompt}<|end|>\n",
    "    <|assistant|>\"\"\" # Termina com assistant, pois a próxima resposta será do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e5fa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    output = pipe(template(prompt), **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b82ce",
   "metadata": {},
   "source": [
    "A tag **<|end|>** é utilizada para delimitar o fim do texto, isto resolverá o problema de gerar mais textos após o modelo dar a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "858b8a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\n    You are a helpful assistant.<|end|>\\n    <|user|>\\n    Quem foi a primeira pessoa no espaço?<|end|>\\n    <|assistant|>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c206565",
   "metadata": {},
   "source": [
    "O problema da geração de novos textos sem contexto com a pergunta foi resolvido! Agora o modelo não gera mais textos desnecessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d61e7359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sim, eu entendo o português. Como posso ajudar você hoje?\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"Você entende de português?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed36339",
   "metadata": {},
   "source": [
    "### Explorando a engenheraria de prompt\n",
    "\n",
    "A engenharia de prompt tem o objetivo de gerar melhores textos para garantir uma melhor comunicação com uma inteligência artificial.\n",
    "\n",
    "Quando você estiver trabalhando com um problema específico e não estiver obtendo os resultados desejados, é sempre importante conferir se o prompt pode ser mais específico, pois as vezes é necessário enviar mais detalhes, tanto na pergunta quanto no prompt de sistema.\n",
    "\n",
    "Se mesmo com a melhoria dos prompts e testes com diferentes parâmetros (*generation_args*) o modelo não apresentar resultados satisfatórios, talvez o modelo não seja adequado para o problema. Portanto, é sempre interessante testar diferentes prompts, juntamente com modelos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e74cc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_sys_prompt(prompt, sys_prompt):\n",
    "    return f\"\"\"<|system|>\n",
    "    {sys_prompt}<|end|>\n",
    "    <|user|>\n",
    "    {prompt}<|end|>\n",
    "    <|assistant|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b674455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_sys_prompt(prompt, sys_prompt):\n",
    "    output = pipe(template_sys_prompt(prompt, sys_prompt), **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9914b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "    Você é um assistente virtual prestativo. Responda as perguntas em português.<|end|>\n",
      "    <|user|>\n",
      "    O que é IA?<|end|>\n",
      "    <|assistant|>\n",
      " A IA, ou Inteligência Artificial, é um campo da ciência da computação que se dedica ao desenvolvimento de sistemas capazes de realizar tarefas que normalmente exigem inteligência humana. Essas tarefas incluem aprendizado, raciocínio, percepção e resolução de problemas. A IA pode ser categorizada em diferentes tipos, como IA fraca (ou aprendizagem de máquina) e IA forte, que ainda está em desenvolvimento. A IA é aplicada em diversos setores, como saúde, finanças, automação e entretenimento.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O que é IA?\"\n",
    "sys_prompt = \"Você é um assistente virtual prestativo. Responda as perguntas em português.\"\n",
    "print(template_sys_prompt(prompt, sys_prompt))\n",
    "print(llm_sys_prompt(prompt, sys_prompt)) # Exibe a resposta do modelo em sequência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31027a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "    Você é um programador experiente. Retorne o código requisitado e forneça explicações breves se achar conveniente<|end|>\n",
      "    <|user|>\n",
      "    Gere um código em python que escreva a sequência de fibonnaci<|end|>\n",
      "    <|assistant|>\n",
      " ```python\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "    if n <= 0:\n",
      "\n",
      "        return []\n",
      "\n",
      "    elif n == 1:\n",
      "\n",
      "        return [0]\n",
      "\n",
      "    elif n == 2:\n",
      "\n",
      "        return [0, 1]\n",
      "\n",
      "\n",
      "    fib_sequence = [0, 1]\n",
      "\n",
      "    for i in range(2, n):\n",
      "\n",
      "        next_number = fib_sequence[i-1] + fib_sequence[i-2]\n",
      "\n",
      "        fib_sequence.append(next_number)\n",
      "\n",
      "    return fib_sequence\n",
      "\n",
      "\n",
      "# Exemplo de uso:\n",
      "\n",
      "n = 10  # Quantidade de números da sequência de Fibonacci que queremos gerar\n",
      "\n",
      "print(fibonacci(n))\n",
      "\n",
      "```\n",
      "\n",
      "Este código define uma função `fibonacci` que recebe um número `n` e retorna uma lista com os primeiros `n` números da sequência de Fibonacci. A sequência começa com 0 e 1, e cada número subsequente é a soma dos dois anteriores.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Gere um código em python que escreva a sequência de fibonnaci\"\n",
    "sys_prompt = \"Você é um programador experiente. Retorne o código requisitado e forneça explicações breves se achar conveniente\"\n",
    "print(template_sys_prompt(prompt, sys_prompt))\n",
    "print(llm_sys_prompt(prompt, sys_prompt)) # Exibe a resposta do modelo em sequência"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8fdff",
   "metadata": {},
   "source": [
    "### Formato de mensagens\n",
    "\n",
    "- **Role system**: Indica o que nós estamos especificando sobre o modelo (sys_prompt).\n",
    "- **Role user**: Indica que a mensagem é do usuário.\n",
    "\n",
    "Em resumo, essa é uma maneira alternativa para obtermos o mesmo resultado que já haviamos trabalhado, porém sem a utilização do *template*. Com isso, o código fica um pouco mais limpo. É apenas uma maneira alternativa e mais fácil se comparado com a utilização do template.\n",
    "\n",
    "Quando utilizamos esse tipo de formato de mensagens (assim como com templates), nós garantimos que o modelo não vai cometer alucinações e gerar textos desnecessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "050c7730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A IA, ou Inteligência Artificial, é um campo da ciência da computação que se dedica ao desenvolvimento de sistemas capazes de realizar tarefas que normalmente exigem inteligência humana. Essas tarefas incluem aprendizado, raciocínio, percepção e resolução de problemas. A IA pode ser categorizada em diferentes tipos, como IA fraca (ou aprendizagem de máquina) e IA forte (ou inteligência generalizada), e é aplicada em diversos setores, como saúde, finanças, automação e entretenimento.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O que é IA?\"\n",
    "msg = [\n",
    "    {\"role\": \"system\", \"content\": \"Você é um assistente virtual prestativo. Responda as perguntas em português.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "output = pipe(msg, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "534981fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_msg(prompt: str, sys_prompt: str):\n",
    "    msg = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    output = pipe(msg, **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "410f5b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. São Paulo\n",
      "\n",
      "2. Rio de Janeiro\n",
      "\n",
      "3. Brasília\n",
      "\n",
      "4. Salvador\n",
      "\n",
      "5. Fortaleza\n",
      "\n",
      "6. Recife\n",
      "\n",
      "7. Belo Horizonte\n",
      "\n",
      "8. Curitiba\n",
      "\n",
      "9. Porto Alegre\n",
      "\n",
      "10. Goiânia\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Liste o nome de 10 cidades famosas do Brasil\"\n",
    "sys_prompt = \"Você é um assistente virtual prestativo. Responda as perguntas em português.\"\n",
    "\n",
    "print(llm_msg(prompt, sys_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5284c",
   "metadata": {},
   "source": [
    "### Otimizando com Quantização (Não funcionou tão bem)\n",
    "\n",
    "A **quantização** reduz a precisão dos números usados para representar os parâmetros do modelo, diminuindo o uso de memória e os requisitos computacionais.\n",
    "\n",
    "Ao invés de usar números de ponto flutuante de 32 bits (float32), o modelo pode usar números de 16 bits (float16) ou até de 8 bits (int8).\n",
    "\n",
    "Este processo pode reduzir significativamente o **tamanho do modelo** e acelerar a inferência sem causar uma perda substancial na precisão.\n",
    "\n",
    "**Benefícios para LLMs:**\n",
    "- Permite rodar grandes modelos em hardware com recursos limitados.\n",
    "- Mantém um bom desempenho sem comprometer significativamente a precisão.\n",
    "- Facilita a execução de LLMs em dispositivos móveis e sistemas em tempo real.\n",
    "- Ideal para quem usa platataforma como o Google Colab com recursos gratuitos.\n",
    "\n",
    "No nosso exemplo inicial, usaremos a quantização de 4 bits do módulo *BitsAndBytesConfig* da biblioteca Transformers (já importado no início do código).\n",
    "\n",
    "Todos os parâmetros abaixo são recomendados na documentação do modelo para obter o melhor desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "078bba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Os pesos do modelo são carregados com esta precisão (4bits)\n",
    "    bnb_4bit_quant_type=\"nf4\", # Especifica o tipo de quantização de 4 bits a ser usado (NF: Normal Float 4), é um esquema de quantização que ajuda a manter o desempenho do modelo enquanto reduz a precisão\n",
    "    bnb_4bit_use_double_quant=True, # Habilita a quantização dupla e reduz o erro de quantização, melhora a estabilidade do modelo\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Define o tipo de dados para cálculo, bfloat16 significa Brain Floating Point, melhora a eficiência computacional mantendo a maior parte da precisão dos números de ponto flutuante de 32 bits\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709960f",
   "metadata": {},
   "source": [
    "Iremos utilizar um modelo mais complexo, o Llama!\n",
    "\n",
    "É necessário aceitar os termos de uso e preencher alguns dados antes de baixar o modelo.\n",
    "\n",
    "[Documentação Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "\n",
    "**OBS:** Não foi possível realizar o carregamento do modelo Llama com quantização, iremos ver esta parte no próximo módulo com LangChain e Ollama. Portanto, **continuaremos sem a quantização**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43dbcb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "================================================================================\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('unix')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('VSCODE_WSL_EXT_LOCATION/up')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('\"en\",\"defaultMessagesFile\"'), PosixPath('\"en\",\"resolvedLanguage\"'), PosixPath('{}}'), PosixPath('\"en\",\"availableLanguages\"'), PosixPath('\"/home/wolf/.vscode-server/bin/e3a5acfb517a443235981655413d566533107e92/out/nls.messages.json\",\"locale\"'), PosixPath('{\"userLocale\"'), PosixPath('\"en\",\"osLocale\"')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "DEBUG: Possible options found for libcudart.so: set()\n",
      "CUDA SETUP: PyTorch settings found: CUDA_VERSION=128, Highest Compute Capability: 7.5.\n",
      "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "CUDA SETUP: Required library version not found: libbitsandbytes_cuda128.so. Maybe you need to compile it from source?\n",
      "CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n",
      "\n",
      "================================================ERROR=====================================\n",
      "CUDA SETUP: CUDA detection failed! Possible reasons:\n",
      "1. You need to manually override the PyTorch CUDA version. Please see: \"https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "2. CUDA driver not installed\n",
      "3. CUDA not installed\n",
      "4. You have multiple conflicting CUDA libraries\n",
      "5. Required library not pre-compiled for this bitsandbytes release!\n",
      "CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=118`.\n",
      "CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.\n",
      "================================================================================\n",
      "\n",
      "CUDA SETUP: Problem: The main issue seems to be that the main CUDA runtime library was not detected.\n",
      "CUDA SETUP: Solution 1: To solve the issue the libcudart.so location needs to be added to the LD_LIBRARY_PATH variable\n",
      "CUDA SETUP: Solution 1a): Find the cuda runtime library via: find / -name libcudart.so 2>/dev/null\n",
      "CUDA SETUP: Solution 1b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_1a\n",
      "CUDA SETUP: Solution 1c): For a permanent solution add the export from 1b into your .bashrc file, located at ~/.bashrc\n",
      "CUDA SETUP: Solution 2: If no library was found in step 1a) you need to install CUDA.\n",
      "CUDA SETUP: Solution 2a): Download CUDA install script: wget https://raw.githubusercontent.com/TimDettmers/bitsandbytes/main/cuda_install.sh\n",
      "CUDA SETUP: Solution 2b): Install desired CUDA version to desired location. The syntax is bash cuda_install.sh CUDA_VERSION PATH_TO_INSTALL_INTO.\n",
      "CUDA SETUP: Solution 2b): For example, \"bash cuda_install.sh 113 ~/local/\" will download CUDA 11.3 and install into the folder ~/local\n",
      "CUDA SETUP: Setup Failed!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import module 'validate_bnb_backend_availability'. Are this object's requirements defined correctly?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:2302\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2302\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2303\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:2332\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:2330\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cuda_setup, research, utils\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     MatmulLtState,\n\u001b[1;32m      9\u001b[0m     bmm_cublas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     mm_cublas,\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/research/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     matmul_fp8_global,\n\u001b[1;32m      4\u001b[0m     matmul_fp8_mixed,\n\u001b[1;32m      5\u001b[0m     switchback_bnb,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/research/autograd/_functions.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalOutlierPooler, MatmulLtState\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/autograd/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_inverse_transform_indices, undo_layout\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# math.prod not compatible with python < 3.8\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/functional.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pack_dict_to_tensor, unpack_tensor_to_dict\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA, lib\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# math.prod not compatible with python < 3.8\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/bitsandbytes/cextension.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mprint_log_stack()\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001b[39m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m    python -m bitsandbytes\u001b[39m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m     25\u001b[0m _ \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mcadam32bit_grad_fp32  \u001b[38;5;66;03m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Carregando o modelo e em sequência o tokenizador\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m llama \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m tokenizer_llama \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/modeling_utils.py:288\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/modeling_utils.py:5011\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[1;32m   5003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5004\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors.index.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   5005\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   5006\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe transformers file in the config seems to be incorrect: it is neither a safetensors file \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5007\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5008\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_explicit_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5009\u001b[0m         )\n\u001b[0;32m-> 5011\u001b[0m hf_quantizer, config, dtype, device_map \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_quantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\n\u001b[1;32m   5013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   5017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5018\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/quantizers/auto.py:319\u001b[0m, in \u001b[0;36mget_hf_quantizer\u001b[0;34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001b[0m\n\u001b[1;32m    316\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_dtype(dtype)\n\u001b[1;32m    327\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:93\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         )\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n\u001b[1;32m     96\u001b[0m bnb_multibackend_is_enabled \u001b[38;5;241m=\u001b[39m is_bitsandbytes_multi_backend_available()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:2305\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2303\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2304\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m   2306\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Are this object\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms requirements defined correctly?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2307\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2309\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m   2310\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Could not import module 'validate_bnb_backend_availability'. Are this object's requirements defined correctly?"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Carregando o modelo e em sequência o tokenizador\n",
    "llama = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             quantization_config=quantization_config)\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336fbdaf",
   "metadata": {},
   "source": [
    "Não deu muito certo, o modelo alucionou. Talvez o modo mais simples seria fazer com as formas anteriores (considerando o modelo Phi-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a7c0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Quem foi a primeira pessoa no espaço?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Você é um assistente virtual prestativo. Responda as perguntas em português\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1741646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse modelo já faz a criação dos templates diretamente na função\n",
    "encodeds = tokenizer.apply_chat_template(messages,\n",
    "                                               return_tensors=\"pt\") # pt significa que os tensores retornados devem estar no formato da bibliteca PyTorch (os pesos tem extensão .pt, não tem relação com o idioma)\n",
    "\n",
    "model_inputs = encodeds.to(device) # Move os tensores codificados para o dispositivo (cuda:0 ou cpu)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=500, do_sample=True,\n",
    "                               pad_token_id=tokenizer.eos_token_id) # Define o token de padding para ser o token de fim de sequência, garante que o modelo não gere textos além da pergunta que nós especificamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a62f7870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> Você é um assistente virtual prestativo. Responda as perguntas em português<|end|><|user|> Quem foi a primeira pessoa no espaço?<|end|><|endoftext|> \n",
      "\n",
      "\n",
      "Grade: grade-2 science\n",
      "\n",
      "Topic: How do flowering plants make new plants?\n",
      "\n",
      "Keyword: flowers\n",
      "\n",
      "\n",
      "Exercise:\n",
      "Complete the following sentences related to reproduction in plants using the words: bee, egg, and seed.\n",
      "- When pollen travels from one flower to another, a bee plays a crucial role because it is the __________.\n",
      "- After a flower is pollinated, it can start forming a __________, which will grow into a new plant.\n",
      "- These tiny structures that can sprout into a new plant are called __________.\n",
      "\n",
      "Solution:\n",
      "1. When pollen travels from one flower to another, a bee plays a crucial role because it is important for its pollination. Pollination is the process through which pollen (the very fine yellow powder that comes from the male part of the flower) travels to the female part of the same or another flower. Bees, and other animals like butterflies, are great helpers in this process because as they move from flower to flower searching for nectar, which is a sweet liquid they love to drink, they accidentally pick up pollen on their bodies. When they visit another flower, some of that pollen rubs off onto the female part, called the stigma, which then allows the process of making seeds to begin. Bees are like nature's important messengers for plants!\n",
      "\n",
      "2. After a flower is pollinated, it can start forming a fruit, which will grow into a new plant. In many plants, the fruit actually contains the seeds, which means it's not just a tasty snack for us or animals, but it's where the next generation of plants will eventually come from. Inside that fruit, the seeds are protected while they're growing, and after they're all mature, they can be spread in many different ways to grow into new flowering plants.\n",
      "\n",
      "3. These tiny structures that can sprout into a new plant are called seeds. Seeds are like little packages that have everything a baby plant needs to start its life—yep, they’re very special indeed! They come from the flowers we've pollinated. Inside each seed is a tiny baby plant, which needs to start in the\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "res = decoded[0]\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
