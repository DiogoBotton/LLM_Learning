{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064eeede",
   "metadata": {},
   "source": [
    "### Introdução ao uso de LangChain\n",
    "\n",
    "O Langchain permite trabalhar facilmente com diversos modelos.\n",
    "\n",
    "**Modelos:**\n",
    "\n",
    "- [Documentação LangChain](https://python.langchain.com/docs/integrations/llms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c4f42",
   "metadata": {},
   "source": [
    "### Instalação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ca88d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolf/anaconda3/envs/ai_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-21 23:31:39.238491: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761100299.261633   21584 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761100299.268628   21584 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761100299.286685   21584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761100299.286708   21584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761100299.286709   21584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761100299.286711   21584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-21 23:31:39.291732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import getpass # Para informar o token do huggingFace\n",
    "\n",
    "# Transformers do HuggingFace\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig, AwqConfig\n",
    "\n",
    "# Integrando Langchain juntamente com o Hugging Face\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    PromptTemplate\n",
    ")\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cace9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72323438",
   "metadata": {},
   "source": [
    "### Carregando LLM via pipeline (a mesma da aula anterior)\n",
    "\n",
    "**Utilizando Langchain juntamente com o HuggingFace**\n",
    "\n",
    "Aqui iremos utilizar o mesmo código e modelo da aula anterior, porém, utilizando o Langchain para pré processar os textos, sem a necessidade de criar templates como fizemos nas aulas com HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0672842",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d7be1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:01<00:00, 60.75s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, # Modelo que será baixado\n",
    "    device_map = \"cuda\", # Deve ser carregada em uma GPU habilitada para CUDA\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = False,\n",
    "    attn_implementation=\"eager\" # Método de implementação para o mecanismo de atenção\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174be56a",
   "metadata": {},
   "source": [
    "A temperatura controla a aleatoriedade do processo de geração de texto.\n",
    "\n",
    "Valores mais baixos tornam a saída do modelo mais determinística, enquanto que valores mais altos aumentam a \"criatividade\" do algoritimo (algo que também pode causar a alucinação)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ec16cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    task = \"text-generation\",\n",
    "    temperature = 0.1,\n",
    "    max_new_tokens = 500, # Será gerado no máximo 500 tokens\n",
    "    do_sample = True, # Adiciona fator de aleatoriedade na geração dos textos\n",
    "    repetition_penalty = 1.1, # Desencoraja o modelo a gerar textos ou frases muito repetitivas. 1 é padrão (nenhuma penalidade)\n",
    "    return_full_text = False # Determina se deve ser retornado o texto completo, incluindo o prompt completo ou apenas o texto gerado\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bac001d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "884588cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Response:O significado ou propósito da vida tem sido objeto de debate filosófico, espiritual e científico por séculos. A resposta a essa pergunta pode variar muito dependendo das crenças pessoais, culturais e religiosas individuais. Algumas pessoas encontram significado em suas conexões com outras pessoas, contribuições para sua comunidade ou realização de seus próprios sonhos e metas. Outras buscam significado através do crescimento pessoal, autoaperfeiçoamento ou experiências espirituais profundas. Em última análise, muitos argumentam que cada indivíduo deve criar seu próprio senso de significado na vida, explorando as paixões, valores e interesses únicos deles mesmos.\n"
     ]
    }
   ],
   "source": [
    "output = llm.invoke(input=\"Qual é o sentido da vida?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a6ca5",
   "metadata": {},
   "source": [
    "### Outros modelos Open Source (Llama)\n",
    "\n",
    "- [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "\n",
    "Com a máquina utilizada para fazer o curso, não foi possível baixar e utilizar o modelo llama a partir do HuggingFace nem com a versão quantizada, iremos continuar utilizando o modelo *Phi-3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68021d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9778c6a",
   "metadata": {},
   "source": [
    "### Modelos de Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dbbf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (HumanMessage, SystemMessage)\n",
    "from langchain_huggingface import ChatHuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd309f",
   "metadata": {},
   "source": [
    "#### **Adequando um modelo**\n",
    "\n",
    "Aqui estamos definindo como o modelo irá se comportar. Diferentemente dos exemplos onde precisavamos definir as tags de finalização dos textos e quem era o ator (role) manualmente, desta vez podemos definir com as classes *SystemMessage* e *HumanMessage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27c7c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [\n",
    "    SystemMessage(content=\"Você é um assistente e está respondendo perguntas gerais.\"),\n",
    "    HumanMessage(content=\"Explique para mim brevemente o que seria a vida.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c7ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatHuggingFace(llm=llm) # Aqui é definido qual LLM será utilizada, no momento, estamos utilizando o Phi-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc5d66",
   "metadata": {},
   "source": [
    "Abaixo é o template do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86f8bfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_template = tokenizer.chat_template\n",
    "model_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd54761",
   "metadata": {},
   "source": [
    "Abaixo é o template preenchido com os prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a28dd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nVocê é um assistente e está respondendo perguntas gerais.<|end|>\\n<|user|>\\nExplique para mim brevemente o que seria a vida.<|end|>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model._to_chat_prompt(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09e8c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A vida, em seu sentido mais amplo, refere-se à existência de seres vivos - organismos capazes de crescer, se reproduzir, responder ao ambiente e manter uma ordem interna por meio do metabolismo. Ela abrange desde as formas simples como bactérias até os complexos sistemas biológicos encontrados nas plantas e animais. Cada forma de vida possui características únicas adaptadas aos seus habitats específicos, permitindo sua sobrevivência e evolução ao longo do tempo geológico. Em essência, a vida representa toda a diversidade da natureza viva presente no planeta Terra.\n"
     ]
    }
   ],
   "source": [
    "resp = chat_model.invoke(msgs)\n",
    "print(resp.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
