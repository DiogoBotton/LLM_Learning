{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064eeede",
   "metadata": {},
   "source": [
    "### Introdução ao uso de LangChain\n",
    "\n",
    "O Langchain permite trabalhar facilmente com diversos modelos.\n",
    "\n",
    "**Modelos:**\n",
    "\n",
    "- [Documentação LangChain](https://python.langchain.com/docs/integrations/llms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c4f42",
   "metadata": {},
   "source": [
    "### Instalação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ca88d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-22 09:10:25.161037: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-22 09:10:29.405914: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-22 09:10:32.604851: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_6336/3038791409.py\", line 6, in <module>\n",
      "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig, AwqConfig\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2302, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2330, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/transformers/pipelines/__init__.py\", line 26, in <module>\n",
      "    from ..image_processing_utils import BaseImageProcessor\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/transformers/image_processing_utils.py\", line 22, in <module>\n",
      "    from .image_transforms import center_crop, normalize, rescale\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/transformers/image_transforms.py\", line 48, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/tensorflow/__init__.py\", line 468, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/keras/__init__.py\", line 7, in <module>\n",
      "    from keras import _tf_keras as _tf_keras\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/keras/_tf_keras/__init__.py\", line 1, in <module>\n",
      "    from keras._tf_keras import keras\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/keras/_tf_keras/keras/__init__.py\", line 7, in <module>\n",
      "    from keras import activations as activations\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/keras/activations/__init__.py\", line 7, in <module>\n",
      "    from keras.src.activations import deserialize as deserialize\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/keras/src/__init__.py\", line 13, in <module>\n",
      "    from keras.src import visualization\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/keras/src/visualization/__init__.py\", line 1, in <module>\n",
      "    from keras.src.visualization import draw_bounding_boxes\n",
      "  File \"/home/wolf/anaconda3/envs/spero-env/lib/python3.11/site-packages/keras/src/visualization/draw_bounding_boxes.py\", line 11, in <module>\n",
      "    import cv2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import getpass # Para informar o token do huggingFace\n",
    "\n",
    "# Transformers do HuggingFace\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig, AwqConfig\n",
    "\n",
    "# Integrando Langchain juntamente com o Hugging Face\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    PromptTemplate\n",
    ")\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cace9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72323438",
   "metadata": {},
   "source": [
    "### Carregando LLM via pipeline (a mesma da aula anterior)\n",
    "\n",
    "**Utilizando Langchain juntamente com o HuggingFace**\n",
    "\n",
    "Aqui iremos utilizar o mesmo código e modelo da aula anterior, porém, utilizando o Langchain para pré processar os textos, sem a necessidade de criar templates como fizemos nas aulas com HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0672842",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7be1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.65s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, # Modelo que será baixado\n",
    "    device_map = \"cuda\", # Deve ser carregada em uma GPU habilitada para CUDA\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = False,\n",
    "    attn_implementation=\"eager\" # Método de implementação para o mecanismo de atenção\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174be56a",
   "metadata": {},
   "source": [
    "A temperatura controla a aleatoriedade do processo de geração de texto.\n",
    "\n",
    "Valores mais baixos tornam a saída do modelo mais determinística, enquanto que valores mais altos aumentam a \"criatividade\" do algoritimo (algo que também pode causar a alucinação)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ec16cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    task = \"text-generation\",\n",
    "    temperature = 0.1,\n",
    "    max_new_tokens = 500, # Será gerado no máximo 500 tokens\n",
    "    do_sample = True, # Adiciona fator de aleatoriedade na geração dos textos\n",
    "    repetition_penalty = 1.1, # Desencoraja o modelo a gerar textos ou frases muito repetitivas. 1 é padrão (nenhuma penalidade)\n",
    "    return_full_text = False # Determina se deve ser retornado o texto completo, incluindo o prompt completo ou apenas o texto gerado\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bac001d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "884588cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Response:O significado ou propósito da vida tem sido objeto de debate filosófico, espiritual e científico por séculos. A resposta a essa pergunta pode variar muito dependendo das crenças pessoais, culturais e religiosas individuais. Alguns argumentam que cada indivíduo deve encontrar seu próprio significado na vida através do auto-desenvolvimento, contribuição para os outros e busca de experiências gratificantes. Outros sugerem que um senso coletivo de bem estar social, harmonia comunitária e responsabilidade ambiental são aspectos importantes no contexto mais amplo da existência humana. Em última análise, muitos concordam que a busca pelo entendimento sobre o significado da vida é uma jornada contínua em vez de um destino final.\n"
     ]
    }
   ],
   "source": [
    "output = llm.invoke(input=\"Qual é o sentido da vida?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a6ca5",
   "metadata": {},
   "source": [
    "### Outros modelos Open Source (Llama)\n",
    "\n",
    "- [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "\n",
    "Com a máquina utilizada para fazer o curso, não foi possível baixar e utilizar o modelo llama a partir do HuggingFace nem com a versão quantizada, iremos continuar utilizando o modelo *Phi-3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68021d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9778c6a",
   "metadata": {},
   "source": [
    "### Modelos de Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dbbf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (HumanMessage, SystemMessage)\n",
    "from langchain_huggingface import ChatHuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd309f",
   "metadata": {},
   "source": [
    "#### Adequando um modelo\n",
    "\n",
    "Aqui estamos definindo como o modelo irá se comportar. Diferentemente dos exemplos onde precisavamos definir as tags de finalização dos textos e quem era o ator (role) manualmente, desta vez podemos definir com as classes *SystemMessage* e *HumanMessage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c7c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [\n",
    "    SystemMessage(content=\"Você é um assistente e está respondendo perguntas gerais.\"),\n",
    "    HumanMessage(content=\"Explique para mim brevemente o que seria a vida.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79c7ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatHuggingFace(llm=llm) # Aqui é definido qual LLM será utilizada, no momento, estamos utilizando o Phi-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc5d66",
   "metadata": {},
   "source": [
    "Abaixo é o template do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f8bfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_template = tokenizer.chat_template\n",
    "model_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd54761",
   "metadata": {},
   "source": [
    "Abaixo é o template preenchido com os prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a28dd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nVocê é um assistente e está respondendo perguntas gerais.<|end|>\\n<|user|>\\nExplique para mim brevemente o que seria a vida.<|end|>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model._to_chat_prompt(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09e8c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A vida, em sua essência mais simples, refere-se à existência de seres vivos - organismos capazes de crescer, se reproduzir, responder aos estímulos do ambiente e manter uma ordem interna através da homeostase. É caracterizada por processos biológicos complexos como metabolismo, genética e adaptação ao longo das gerações. Em termos filosóficos ou espirituais, pode abranger também as experiências humanas, incluindo consciência, emoção e autoconsciência.\n",
      "\n",
      "\n",
      "A vida não apenas existe no nível físico com células vibrantes dentro dos corpos; ela transcende até envolver aspectos culturais, sociais e pessoais que definem quem somos e como interagimos entre si na sociedade humana. Ela é marcada pela busca pelo bem-estar, desenvolvimento individual e coletivo, além da contribuição para comunidades maiores e legados duradouros.\n"
     ]
    }
   ],
   "source": [
    "resp = chat_model.invoke(msgs)\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90341f6",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "Os modelos de prompt (promt templates) ajudam a traduzir a entrada e os parâmetros do usuário em instruções para um modelo de linguagem. Pode ser usado para orientar a resposta de um modelo, ajudando-o a entender o contexto e gerar uma saída relevante. Também facilita a criação de prompts de maneiras variáveis.\n",
    "\n",
    "Existem alguns tipos diferentes de modelos de prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66624340",
   "metadata": {},
   "source": [
    "#### String PromptTemplates\n",
    "\n",
    "Esses modelos de prompt são usados para formatar uma única string e geralmente são usados para entradas mais simples.\n",
    "\n",
    "Tipo de template utilizada para enviar uma string como parâmetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c14962bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Escreva um poema sobre abacates')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Escreva um poema sobre {topic}\")\n",
    "prompt_template.invoke({\"topic\": \"abacates\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b58ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Escreva um poema sobre {topic}')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705a192",
   "metadata": {},
   "source": [
    "#### ChatPromptTemplates\n",
    "\n",
    "São usados para formatar uma lista de mensagens. Ou seja, uma lista de templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11ee6862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Você é um assistente e está respondendo perguntas gerais.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explique-me em 1 parâgrafo o conceito de IA', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages({\n",
    "    (\"system\", \"Você é um assistente e está respondendo perguntas gerais.\"),\n",
    "    (\"user\", \"Explique-me em 1 parâgrafo o conceito de {topic}\")\n",
    "})\n",
    "\n",
    "prompt.invoke({\"topic\": \"IA\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spero-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
